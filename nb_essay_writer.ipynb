{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334cee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get Azure OpenAI configuration from environment variables\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "azure_openai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "azure_openai_embedding_deployment_name = os.getenv(\"AZURE_OPENAI_EmBEDDING_DEPLOYMENT_NAME \")\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=azure_openai_deployment_name,\n",
    "    api_version=azure_openai_api_version,\n",
    "    api_key=azure_openai_api_key,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "#create a tavily api key and use in environment file\n",
    "tavily = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))\n",
    "\n",
    "#create checkpoints\n",
    "sqlite_conn = sqlite3.connect('checkpoint.sqlite', check_same_thread=False)\n",
    "memory = SqliteSaver(sqlite_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3567912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b753fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0841c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e338738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT), \n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23f1e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_plan_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34616fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "        ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content, \n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a763eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67b181fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb78f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bae9609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1eace3c4440>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c198792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1eace3c4440>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0db7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e72af40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': \"**Essay Outline: The Difference Between LangChain and LangSmith**\\n\\n---\\n\\n### **I. Introduction**\\n   - **Purpose of the Essay**: To explore and clarify the differences between LangChain and LangSmith, two tools in the AI and machine learning ecosystem.\\n   - **Context**: Both LangChain and LangSmith are frameworks/tools designed to work with large language models (LLMs), but they serve distinct purposes and are optimized for different use cases.\\n   - **Thesis Statement**: While LangChain focuses on building and managing applications powered by LLMs, LangSmith is designed to enhance the development, debugging, and evaluation of these applications, making them complementary tools in the AI developer's toolkit.\\n\\n---\\n\\n### **II. Overview of LangChain**\\n   - **Definition and Purpose**: \\n     - LangChain is a framework for building applications that leverage LLMs.\\n     - It provides tools to manage prompts, memory, chains, and agents.\\n   - **Key Features**:\\n     - **Prompt Management**: Tools for creating and managing reusable prompts.\\n     - **Chains**: Pipelines for combining multiple LLM calls and other functions.\\n     - **Memory**: Persistent state management for conversational AI.\\n     - **Agents**: Systems that use LLMs to make decisions and interact with external tools.\\n   - **Use Cases**:\\n     - Chatbots, question-answering systems, summarization tools, and more.\\n   - **Strengths**:\\n     - Modular design for flexibility.\\n     - Rich ecosystem of integrations with APIs, databases, and other tools.\\n\\n---\\n\\n### **III. Overview of LangSmith**\\n   - **Definition and Purpose**:\\n     - LangSmith is a platform for debugging, testing, and evaluating applications built with LLMs.\\n     - It focuses on improving the reliability and performance of AI applications.\\n   - **Key Features**:\\n     - **Debugging Tools**: Identify and resolve issues in LLM-based workflows.\\n     - **Evaluation Metrics**: Measure the quality and effectiveness of LLM outputs.\\n     - **Experimentation**: Test different prompts, models, and configurations.\\n     - **Traceability**: Track and analyze the behavior of LLM applications over time.\\n   - **Use Cases**:\\n     - Developers refining LLM-based applications.\\n     - Teams ensuring consistent and reliable AI performance.\\n   - **Strengths**:\\n     - Focus on quality assurance.\\n     - Tools for iterative development and optimization.\\n\\n---\\n\\n### **IV. Key Differences Between LangChain and LangSmith**\\n   - **Primary Focus**:\\n     - LangChain: Building and managing LLM-powered applications.\\n     - LangSmith: Debugging, testing, and evaluating LLM applications.\\n   - **Target Audience**:\\n     - LangChain: Developers creating new AI applications.\\n     - LangSmith: Developers refining and optimizing existing applications.\\n   - **Core Functionality**:\\n     - LangChain: Provides building blocks for application development.\\n     - LangSmith: Offers tools for quality assurance and performance improvement.\\n   - **Integration**:\\n     - LangSmith can be used alongside LangChain to enhance the development process.\\n\\n---\\n\\n### **V. Complementary Nature of LangChain and LangSmith**\\n   - **How They Work Together**:\\n     - LangChain can be used to build applications, while LangSmith can be used to debug and optimize them.\\n     - Example: A developer uses LangChain to create a chatbot and LangSmith to test and refine its responses.\\n   - **Benefits of Using Both**:\\n     - Faster development cycles.\\n     - Higher-quality applications.\\n     - Improved user experience and reliability.\\n\\n---\\n\\n### **VI. Conclusion**\\n   - **Summary of Key Points**:\\n     - LangChain and LangSmith serve distinct but complementary roles in the AI development lifecycle.\\n     - LangChain is focused on building applications, while LangSmith is focused on improving and evaluating them.\\n   - **Final Thoughts**:\\n     - Together, these tools empower developers to create robust, reliable, and high-performing LLM-based applications.\\n   - **Call to Action**:\\n     - Developers should consider incorporating both tools into their workflows to maximize efficiency and effectiveness.\\n\\n---\\n\\n### **Notes for Writing the Essay**:\\n1. **Tone**: Maintain a professional and informative tone throughout the essay.\\n2. **Examples**: Include real-world or hypothetical examples to illustrate the use cases of LangChain and LangSmith.\\n3. **Clarity**: Avoid overly technical jargon; explain concepts in a way that is accessible to readers with a basic understanding of AI.\\n4. **Comparison Table (Optional)**: Consider adding a table to visually summarize the differences between LangChain and LangSmith.\\n5. **Sources**: Reference official documentation or credible sources for accuracy, if applicable.\"}}\n",
      "{'research_plan': {'content': ['# LangChain VS LangSmith LangChain ### LangChain | Tag | Data Pipelines,infrastructure,Data Analysis,LLMs,Data Integration,SQL Assistant,Web Scraper,Multi-Agent Framework | | Tag | Robots,Time Management,LLMs,Contract Management,Development,Data Analysis,Reputation Management | ### LangChain Rank/Visit | Country | United States | #### Top 5 Countries ### LangSmith Rank/Visit | Country | United States | n8n LangChain Integration - Optimize LLM model interactions, create advanced chatbots, personalized assistants, and more with LangChain, a versatile AI software for enhanced performance. Open-source Python, visualize AI reasoning, LangChain, OpenAI & more. DataChain - DataChain is an open-source developer tool that connects unstructured data in cloud storage with AI models and APIs, providing instant insights and dataset versioning. Langflow - Langflow is an open-source Python framework for building multi-agent & RAG apps.', 'May 16, 2020 · Published by Shawn Brink Category: Apps & Features 16 May 2020 How to Install or Uninstall Microsoft WordPad in Windows 10 Microsoft WordPad is a basic rich text editor …', 'Jul 3, 2021 · How to Turn Windows Features On or Off in Windows 10 Information Some programs and features included with Windows, such as Internet Infor']}}\n",
      "{'generate': {'draft': '### The Difference Between LangChain and LangSmith\\n\\nIn the rapidly evolving field of artificial intelligence (AI), developers are constantly seeking tools to streamline the creation and optimization of applications powered by large language models (LLMs). Two such tools, LangChain and LangSmith, have emerged as key players in this space. While both are designed to work with LLMs, they serve distinct purposes and cater to different stages of the AI development lifecycle. LangChain focuses on building and managing LLM-powered applications, whereas LangSmith is designed to enhance the debugging, testing, and evaluation of these applications. Together, they form a complementary toolkit for developers aiming to create robust and reliable AI solutions.\\n\\nLangChain is a versatile framework that provides developers with the building blocks needed to create applications powered by LLMs. Its primary purpose is to simplify the development process by offering tools for managing prompts, memory, chains, and agents. For instance, LangChain’s prompt management feature allows developers to create reusable templates, while its chain functionality enables the combination of multiple LLM calls into cohesive workflows. Additionally, LangChain supports memory management, which is particularly useful for conversational AI applications that require context persistence. With its modular design and rich ecosystem of integrations, LangChain is ideal for building chatbots, question-answering systems, summarization tools, and other AI-driven applications.\\n\\nOn the other hand, LangSmith is a platform tailored for refining and optimizing LLM-based applications. Its primary focus is on improving the reliability and performance of these applications through debugging, testing, and evaluation. LangSmith offers a suite of tools that allow developers to identify and resolve issues in their workflows, measure the quality of LLM outputs, and experiment with different prompts, models, and configurations. One of its standout features is traceability, which enables developers to track and analyze the behavior of their applications over time. This makes LangSmith particularly valuable for teams that prioritize quality assurance and iterative development. By providing insights into the strengths and weaknesses of an application, LangSmith helps developers ensure consistent and reliable AI performance.\\n\\nThe key differences between LangChain and LangSmith lie in their primary focus, target audience, and core functionality. LangChain is geared toward developers who are building new AI applications, offering foundational tools for application development. In contrast, LangSmith is designed for developers who are refining and optimizing existing applications, providing tools for quality assurance and performance improvement. While LangChain emphasizes creation and integration, LangSmith focuses on evaluation and debugging. Despite these differences, the two tools are highly complementary. For example, a developer might use LangChain to build a chatbot and then leverage LangSmith to test and refine its responses, ensuring a seamless user experience.\\n\\nThe complementary nature of LangChain and LangSmith highlights their potential to work together in the AI development process. By combining the strengths of both tools, developers can achieve faster development cycles, higher-quality applications, and improved user satisfaction. For instance, LangChain can be used to design the architecture of an AI application, while LangSmith can be employed to fine-tune its performance and address any shortcomings. This synergy not only enhances the efficiency of the development process but also ensures that the final product meets the highest standards of reliability and functionality.\\n\\nIn conclusion, LangChain and LangSmith serve distinct but interconnected roles in the AI ecosystem. LangChain excels at building and managing LLM-powered applications, while LangSmith focuses on debugging, testing, and optimizing these applications. Together, they empower developers to create robust, reliable, and high-performing AI solutions. By incorporating both tools into their workflows, developers can maximize efficiency and effectiveness, ultimately delivering better outcomes for users. As the field of AI continues to evolve, the integration of tools like LangChain and LangSmith will be instrumental in shaping the future of intelligent applications.', 'revision_number': 2}}\n",
      "{'reflect': {'critique': '### Critique of Submission\\n\\nYour essay provides a clear and well-structured comparison of LangChain and LangSmith, effectively highlighting their distinct purposes and complementary roles in the AI development lifecycle. The writing is articulate, and the technical concepts are explained in a way that is accessible to readers with a basic understanding of AI tools. However, there are areas where the essay could be improved to enhance its depth, engagement, and overall impact.\\n\\n#### Strengths:\\n1. **Clarity and Structure**: The essay is logically organized, with a clear introduction, body, and conclusion. Each section builds upon the previous one, making the comparison easy to follow.\\n2. **Technical Accuracy**: The descriptions of LangChain and LangSmith are accurate and provide a good overview of their functionalities.\\n3. **Complementary Analysis**: The essay does a good job of explaining how the two tools can work together, emphasizing their synergy in the AI development process.\\n\\n#### Areas for Improvement:\\n1. **Depth of Analysis**: While the essay provides a solid overview, it lacks depth in certain areas. For example, the discussion of LangChain’s modular design and integrations could be expanded to include specific examples of integrations or use cases. Similarly, LangSmith’s traceability feature could be elaborated upon with a concrete example of how it benefits developers.\\n2. **Engagement and Style**: The writing is clear but somewhat dry. Adding anecdotes, real-world examples, or hypothetical scenarios could make the essay more engaging and relatable for readers.\\n3. **Critical Perspective**: The essay is overwhelmingly positive about both tools, which may come across as promotional rather than analytical. Including potential limitations or challenges associated with using LangChain and LangSmith would provide a more balanced perspective.\\n4. **Length and Depth**: While the essay is concise, it could benefit from additional depth in certain sections. For instance, the introduction could briefly touch on the broader context of LLM-powered applications and their growing importance in AI. Similarly, the conclusion could explore future trends or speculate on how tools like LangChain and LangSmith might evolve.\\n\\n#### Recommendations for Improvement:\\n1. **Expand the Introduction**: Provide more context about the significance of LLM-powered applications in AI and why tools like LangChain and LangSmith are necessary. This will help readers understand the broader relevance of the essay.\\n2. **Add Real-World Examples**: Include specific examples of applications built with LangChain (e.g., a chatbot for customer service) and scenarios where LangSmith has been used to debug or optimize workflows. This will make the essay more concrete and relatable.\\n3. **Discuss Limitations**: Address potential challenges or limitations of each tool. For instance, LangChain’s modular design might require a steep learning curve for beginners, or LangSmith’s traceability might involve additional computational overhead.\\n4. **Enhance Style**: Use more engaging language and consider adding a hypothetical scenario to illustrate the synergy between LangChain and LangSmith. For example, describe a developer’s journey in building and refining an AI application using both tools.\\n5. **Expand the Conclusion**: Speculate on the future of AI development tools and how LangChain and LangSmith might adapt to emerging trends, such as multimodal models or real-time applications.\\n6. **Increase Length**: Aim for a word count of 1,200–1,500 words to allow for deeper exploration of the topics. This will give you room to expand on technical details, provide examples, and discuss broader implications.\\n\\n#### Suggested Outline for Revision:\\n1. **Introduction**:\\n   - Broader context: The rise of LLM-powered applications and their impact on AI development.\\n   - Importance of tools like LangChain and LangSmith in streamlining workflows.\\n   - Thesis statement: Overview of their distinct roles and complementary nature.\\n\\n2. **LangChain Overview**:\\n   - Detailed explanation of features (e.g., prompt management, chains, memory).\\n   - Examples of applications built with LangChain.\\n   - Benefits and potential challenges.\\n\\n3. **LangSmith Overview**:\\n   - Detailed explanation of features (e.g., debugging, testing, traceability).\\n   - Examples of how LangSmith has been used to optimize applications.\\n   - Benefits and potential challenges.\\n\\n4. **Comparison and Synergy**:\\n   - Key differences in focus, target audience, and functionality.\\n   - Hypothetical scenario illustrating how the tools can be used together.\\n   - Discussion of how their synergy enhances development efficiency and quality.\\n\\n5. **Conclusion**:\\n   - Recap of their distinct and complementary roles.\\n   - Speculation on future trends in AI development tools.\\n   - Final thoughts on their importance in shaping the future of intelligent applications.\\n\\nBy incorporating these recommendations, your essay will become more comprehensive, engaging, and balanced, providing readers with a deeper understanding of LangChain and LangSmith and their roles in AI development.'}}\n",
      "{'research_critique': {'content': ['# LangChain VS LangSmith LangChain ### LangChain | Tag | Data Pipelines,infrastructure,Data Analysis,LLMs,Data Integration,SQL Assistant,Web Scraper,Multi-Agent Framework | | Tag | Robots,Time Management,LLMs,Contract Management,Development,Data Analysis,Reputation Management | ### LangChain Rank/Visit | Country | United States | #### Top 5 Countries ### LangSmith Rank/Visit | Country | United States | n8n LangChain Integration - Optimize LLM model interactions, create advanced chatbots, personalized assistants, and more with LangChain, a versatile AI software for enhanced performance. Open-source Python, visualize AI reasoning, LangChain, OpenAI & more. DataChain - DataChain is an open-source developer tool that connects unstructured data in cloud storage with AI models and APIs, providing instant insights and dataset versioning. Langflow - Langflow is an open-source Python framework for building multi-agent & RAG apps.', 'May 16, 2020 · Published by Shawn Brink Category: Apps & Features 16 May 2020 How to Install or Uninstall Microsoft WordPad in Windows 10 Microsoft WordPad is a basic rich text editor …', 'Jul 3, 2021 · How to Turn Windows Features On or Off in Windows 10 Information Some programs and features included with Windows, such as Internet Infor', '* How do I integrate LangChain with other AI frameworks? # How do I integrate LangChain with other AI frameworks? Integrating LangChain with other AI frameworks involves leveraging its modular design and compatibility with common tools. You might also use LangChain’s API integration tools to connect to external services like OpenAI, then pass outputs to other frameworks for post-processing. You could wrap it in a LangChain `Tool` class, enabling it to be used within a LangChain agent alongside other components like vector databases or APIs. For instance, an agent could first retrieve relevant documents using LangChain’s document loaders, process them with your PyTorch model, and then generate a summary using OpenAI. Tools like LangChain’s `RunnableLambda` or custom functions allow data transformation between frameworks.', 'This is a cookbook with examples of the Langfuse Integration for Langchain (Python). from langfuse.langchain import CallbackHandler # Initialize Langfuse CallbackHandler for Langchain (tracing) from langchain_openai import ChatOpenAI from langchain.prompts import ChatPromptTemplate chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"}, config={\"callbacks\":[langfuse_handler]}) await chain2.ainvoke({\"person\": \"biden\", \"language\": \"german\"}, config={\"callbacks\":[langfuse_handler]}) chain2.batch([{\"person\": \"elon musk\", \"language\": \"english\"}, {\"person\": \"mark zuckerberg\", \"language\": \"english\"}], config={\"callbacks\":[langfuse_handler]}) await chain2.abatch([{\"person\": \"jeff bezos\", \"language\": \"english\"}, {\"person\": \"tim cook\", \"language\": \"english\"}], config={\"callbacks\":[langfuse_handler]}) for chunk in chain2.stream({\"person\": \"steve jobs\", \"language\": \"english\"}, config={\"callbacks\":[langfuse_handler]}): async for chunk in chain2.astream({\"person\": \"bill gates\", \"language\": \"english\"}, config={\"callbacks\":[langfuse_handler]}): from langchain_community.document_loaders import SeleniumURLLoader from langchain_text_splitters import CharacterTextSplitter from langchain_openai import OpenAIEmbeddings from langchain.chains import RetrievalQA chain.invoke(query, config={\"callbacks\":[langfuse_handler]}) from langchain_openai import AzureChatOpenAI from langchain.prompts import ChatPromptTemplate from langfuse.langchain import CallbackHandler chain.invoke({\"person\": \"Satya Nadella\"}, config={\"callbacks\":[langfuse_handler]})', 'In this post, we’ll explore how LangSmith enables deep observability in your applications through tracing, allowing for a more efficient and transparent development process. *   The`@traceable`decorator logs detailed traces each time the`process_transaction`function is called. *   This metadata will be logged with the trace, allowing you to filter and group runs by these values in LangSmith’s UI. LangSmith offers special processing and rendering for LLM traces. LangSmith empowers developers with unparalleled visibility into their applications, especially when working with LLMs. By leveraging the`@traceable`decorator, adding rich metadata, and using advanced features like tracing context managers and conversational threads, you can optimize the performance, reliability, and transparency of your AI applications.', 'LangSmith is a unified observability & evals platform where teams can debug, test, and monitor AI app performance — whether building with LangChain or not. Evaluate your app by saving production traces to datasets —\\xa0 then score performance with LLM-as-Judge evaluators. Yes, you can log traces to LangSmith using a standard OpenTelemetry client to access all LangSmith features, including tracing, running evals, and prompt engineering. How can LangSmith help with observability and evaluation? LangSmith traces contain the full information of all the inputs and outputs of each step of the application, giving users full visibility into their agent or LLM app behavior. LangSmith also allows users to instantly run evals to assess agent or LLM app performance — including LLM-as-Judge evaluators for auto-scoring and the ability to attach human feedback.', 'LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', 'Compare LangChain, LangSmith, and Orq.ai to discover the best LLM development tools for building, deploying, and optimizing scalable AI applications. As a **comprehensive platform** for LLM product development, LangChain equips software teams with the tools needed to build, test, and deploy LLM-powered solutions at scale. LangChain offers a powerful framework for developing modular **LLM applications**, but like any tool, it comes with advantages and challenges. *   **Challenges in Large-Scale Production Deployments:**LangChain is optimized for **prototyping** and development but may require additional tools for monitoring, **evaluating**, and maintaining performance in **production-ready applications**. While LangChain focuses on the flexibility and modularity required for building LLM-powered applications, LangSmith steps in to offer essential tools for deployment, monitoring, and optimization throughout the production process.']}}\n",
      "{'generate': {'draft': \"### The Difference Between LangChain and LangSmith\\n\\nIn the rapidly evolving field of artificial intelligence (AI), developers are constantly seeking tools to streamline the creation and optimization of applications powered by large language models (LLMs). Two such tools, LangChain and LangSmith, have emerged as essential components in the AI developer's toolkit. While both are designed to work with LLMs, they serve distinct purposes and cater to different stages of the development lifecycle. LangChain focuses on building and managing LLM-powered applications, whereas LangSmith is designed to enhance the debugging, testing, and evaluation of these applications. Together, they offer complementary solutions for creating robust and reliable AI systems.\\n\\nLangChain is a versatile framework that provides developers with the building blocks needed to create applications powered by LLMs. Its modular design allows for flexibility in combining various components, such as prompts, memory, chains, and agents. For instance, LangChain’s prompt management tools enable developers to create reusable templates for interacting with LLMs, while its chain functionality allows for the seamless integration of multiple LLM calls and external functions into a cohesive pipeline. Additionally, LangChain supports persistent memory, which is particularly useful for conversational AI applications that require context retention. Common use cases for LangChain include chatbots, question-answering systems, and summarization tools. Its rich ecosystem of integrations with APIs, databases, and other tools makes it an ideal choice for developers looking to prototype and build LLM-powered applications efficiently.\\n\\nOn the other hand, LangSmith is a platform designed to optimize the performance and reliability of LLM-based applications. It provides developers with advanced tools for debugging, testing, and evaluating their systems. For example, LangSmith’s traceability features allow developers to track and analyze the behavior of their applications, offering insights into how inputs are processed and outputs are generated. This level of visibility is invaluable for identifying and resolving issues in complex workflows. LangSmith also includes evaluation metrics to measure the quality of LLM outputs, enabling developers to experiment with different prompts, models, and configurations. By focusing on quality assurance and iterative development, LangSmith ensures that applications perform consistently and meet user expectations. It is particularly useful for teams refining existing applications or preparing them for large-scale deployment.\\n\\nThe primary difference between LangChain and LangSmith lies in their focus and functionality. LangChain is centered on building and managing applications, providing the foundational tools needed to create LLM-powered solutions. In contrast, LangSmith is geared toward debugging, testing, and optimizing these applications, ensuring they operate reliably in production environments. While LangChain is ideal for developers in the early stages of application development, LangSmith is better suited for those refining and scaling their systems. Additionally, LangSmith’s emphasis on traceability and evaluation makes it a critical tool for maintaining high standards of performance and reliability. Despite these differences, the two tools are highly complementary and can be used together to streamline the entire development lifecycle.\\n\\nThe synergy between LangChain and LangSmith becomes evident when they are used in tandem. For example, a developer might use LangChain to build a chatbot that integrates multiple APIs and databases, leveraging its modular design to create a sophisticated conversational agent. Once the chatbot is functional, the developer can use LangSmith to test its responses, identify areas for improvement, and optimize its performance. This iterative process not only accelerates development cycles but also results in higher-quality applications that deliver a better user experience. By combining the strengths of both tools, developers can create robust, reliable, and scalable AI systems.\\n\\nIn conclusion, LangChain and LangSmith serve distinct but complementary roles in the AI development ecosystem. LangChain provides the tools needed to build and manage LLM-powered applications, while LangSmith focuses on debugging, testing, and optimizing these systems. Together, they empower developers to create high-performing AI applications that meet the demands of modern users. By incorporating both tools into their workflows, developers can maximize efficiency, improve reliability, and deliver exceptional results. As the field of AI continues to evolve, the integration of tools like LangChain and LangSmith will remain essential for driving innovation and excellence in LLM development.\", 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream({\n",
    "    'task': \"what is the difference between langchain and langsmith\",\n",
    "    \"max_revisions\": 2,\n",
    "    \"revision_number\": 1,\n",
    "    \"content\": [],  \n",
    "    \"plan\": \"\",    \n",
    "    \"draft\": \"\",    \n",
    "    \"critique\": \"\"  \n",
    "}, thread):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
